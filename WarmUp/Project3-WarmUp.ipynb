{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up\n",
    "How many unique subreddits are there?\n",
    "\n",
    "Pick a subreddit. What user wrote the most comments in January of 2012? What was the user’s top three most-upvoted comments? Filter out bots or other types of automated posts.\n",
    "\n",
    "Choose a day of significance to you (e.g., your birthday), and retrieve a 5% sample of the comments posted on this particular day across all 5 years of the dataset.\n",
    "\n",
    "The number of comments posted per year will likely trend upward over time as more users join Reddit. However, the popularity of some subreddits may increase or decrease over time. Find An example of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c18d7789a9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://orion11:32001/sampled_reddit/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m columns = [\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"distinguished\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "df = sqlContext.read.json(\"hdfs://orion11:32001/sampled_reddit/*\")\n",
    "columns = [\n",
    "    \"distinguished\",\n",
    "    \"downs\",\n",
    "    \"created_utc\",\n",
    "    \"controversiality\",\n",
    "    \"edited\",\n",
    "    \"gilded\",\n",
    "    \"author_flair_css_class\",\n",
    "    \"id\",\n",
    "    \"author\",\n",
    "    \"retrieved_on\",\n",
    "    \"score_hidden\",\n",
    "    \"subreddit_id\",\n",
    "    \"score\",\n",
    "    \"name\",\n",
    "    \"author_flair_text\",\n",
    "    \"link_id\",\n",
    "    \"archived\",\n",
    "    \"ups\",\n",
    "    \"parent_id\",\n",
    "    \"subreddit\",\n",
    "    \"body\"]\n",
    "\n",
    "df.show(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "How many unique subreddits are there?\n",
    "#### Answer: 253336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"subreddit\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "df = sqlContext.read.json(\"hdfs://orion11:32001/reddit/2012/RC_2012-01.bz2\")\n",
    "columns = [\n",
    "    \"distinguished\",\n",
    "    \"downs\",\n",
    "    \"created_utc\",\n",
    "    \"controversiality\",\n",
    "    \"edited\",\n",
    "    \"gilded\",\n",
    "    \"author_flair_css_class\",\n",
    "    \"id\",\n",
    "    \"author\",\n",
    "    \"retrieved_on\",\n",
    "    \"score_hidden\",\n",
    "    \"subreddit_id\",\n",
    "    \"score\",\n",
    "    \"name\",\n",
    "    \"author_flair_text\",\n",
    "    \"link_id\",\n",
    "    \"archived\",\n",
    "    \"ups\",\n",
    "    \"parent_id\",\n",
    "    \"subreddit\",\n",
    "    \"body\"]\n",
    "\n",
    "df.show(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"created_utc\", df[\"created_utc\"].cast(LongType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Pick a subreddit. What user wrote the most comments in January of 2012?\n",
    "#### Answer: ('Corrupted_Planet', 287)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# January 1st 2012 -> 1325376000\n",
    "#January 31st 2012 -> 1327968000\n",
    "\n",
    "df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "sample_pd = spark.sql(\"\"\"select * from TEMP_DF where temp_df.subreddit = 'runescape' \n",
    "and temp_df.created_utc > 1325376000 \n",
    "and temp_df.created_utc < 1327968000 \n",
    "and temp_df.author != '[deleted]'\"\"\").toPandas()\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(sample_pd.author)\n",
    "counter.most_common()[:5] # get the five most common elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "What was the user’s top three most-upvoted comments? \n",
    "Filter out bots or other types of automated posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pd_2 = spark.sql(\"\"\"select * from TEMP_DF \n",
    "where temp_df.subreddit = 'runescape' \n",
    "and temp_df.created_utc > 1325376000 \n",
    "and temp_df.created_utc < 1327968000 \n",
    "and temp_df.author != '[deleted]'\n",
    "order by temp_df.score desc\"\"\").toPandas()\n",
    "sample_pd_2.iloc[1:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Choose a day of significance to you (e.g., your birthday), and retrieve a 5% sample of the comments posted on this particular day across all 5 years of the dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#12.11.20XX from 00:00:00 to 23:59:59\n",
    "\n",
    "#1512950400 1513036799    2017\n",
    "#1481414400 1481504399    2016\n",
    "#1449792000 1449878399    2015\n",
    "#1418256000 1418342399    2014\n",
    "#1386720000 1386806399    2013\n",
    "#https://www.unixtimestamp.com/index.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "df = sqlContext.read.json(\"hdfs://orion11:32001/sampled_reddit/*\")\n",
    "columns = [\n",
    "    \"distinguished\",\n",
    "    \"downs\",\n",
    "    \"created_utc\",\n",
    "    \"controversiality\",\n",
    "    \"edited\",\n",
    "    \"gilded\",\n",
    "    \"author_flair_css_class\",\n",
    "    \"id\",\n",
    "    \"author\",\n",
    "    \"retrieved_on\",\n",
    "    \"score_hidden\",\n",
    "    \"subreddit_id\",\n",
    "    \"score\",\n",
    "    \"name\",\n",
    "    \"author_flair_text\",\n",
    "    \"link_id\",\n",
    "    \"archived\",\n",
    "    \"ups\",\n",
    "    \"parent_id\",\n",
    "    \"subreddit\",\n",
    "    \"body\"]\n",
    "\n",
    "df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"created_utc\", df[\"created_utc\"].cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "pd_3 = spark.sql(\"\"\"select temp_df.body from TEMP_DF \n",
    "where (temp_df.created_utc > 1512950400 and temp_df.created_utc < 1513036799)\n",
    "or (temp_df.created_utc > 1481414400 and temp_df.created_utc < 1481504399)\n",
    "or (temp_df.created_utc > 1449792000 and temp_df.created_utc < 1449878399)\n",
    "or (temp_df.created_utc > 1418256000 and temp_df.created_utc < 1418342399)\n",
    "or (temp_df.created_utc > 1386720000 and temp_df.created_utc < 1386806399)\"\"\")\n",
    "\n",
    "samp = pd_3.sample(False, .5)\n",
    "samp.write.format('csv').save('hdfs://orion11:32001/sampled_birthday_answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "The number of comments posted per year will likely trend upward over time as more users join Reddit. However, the popularity of some subreddits may increase or decrease over time. Find An example of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "df = sqlContext.read.json(\"hdfs://orion11:32001/reddit/2016/*\")\n",
    "columns = [\n",
    "    \"distinguished\",\n",
    "    \"downs\",\n",
    "    \"created_utc\",\n",
    "    \"controversiality\",\n",
    "    \"edited\",\n",
    "    \"gilded\",\n",
    "    \"author_flair_css_class\",\n",
    "    \"id\",\n",
    "    \"author\",\n",
    "    \"retrieved_on\",\n",
    "    \"score_hidden\",\n",
    "    \"subreddit_id\",\n",
    "    \"score\",\n",
    "    \"name\",\n",
    "    \"author_flair_text\",\n",
    "    \"link_id\",\n",
    "    \"archived\",\n",
    "    \"ups\",\n",
    "    \"parent_id\",\n",
    "    \"subreddit\",\n",
    "    \"body\"]\n",
    "\n",
    "df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "pd_4 = spark.sql(\"\"\"select temp_df.subreddit, MONTH(FROM_UNIXTIME(temp_df.created_utc)) month, \n",
    "count(temp_df.body) comments\n",
    "from TEMP_DF\n",
    "GROUP BY \n",
    "MONTH(FROM_UNIXTIME(temp_df.created_utc)), temp_df.subreddit\"\"\").toPandas()\n",
    "\n",
    "pd_5 = pd_4.pivot_table(index=['subreddit'],\n",
    "                   columns='month',\n",
    "                   values='comments')\n",
    "pd_5.iloc[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
